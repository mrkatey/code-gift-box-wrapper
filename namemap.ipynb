{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhSNWHrjPegv+oknDe1WIy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrkatey/code-gift-box-wrapper/blob/main/namemap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJIF3g_0aw4d",
        "outputId": "1c0b6bd6-8f01-486a-98a1-731f8788bb4a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h3\n",
            "  Downloading h3-3.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h3\n",
            "Successfully installed h3-3.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0oGw7pGYxQV",
        "outputId": "902688d9-23bd-4aa1-f052-8bb58ee75795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Name: Jeff Matkins\n",
            "H3 Index: 8c29a1d752149ff\n"
          ]
        }
      ],
      "source": [
        "import h3\n",
        "\n",
        "# Input data\n",
        "full_name = \"Jeff Matkins\"\n",
        "birth_latitude = 34.05\n",
        "birth_longitude = -118.25\n",
        "\n",
        "# Convert latitude and longitude to H3 hexagon\n",
        "h3_resolution = 12\n",
        "h3_index = h3.geo_to_h3(birth_latitude, birth_longitude, h3_resolution)\n",
        "\n",
        "# Print the H3 index\n",
        "print(\"Full Name:\", full_name)\n",
        "print(\"H3 Index:\", h3_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_coordinates_to_h3(csv_path, resolution):\n",
        "    # Open the input CSV file\n",
        "    with open(csv_path, 'r') as file:\n",
        "        # Read the CSV data\n",
        "        fieldnames = 'phoneme,event,year,latitude,longitude'.split(\",\")\n",
        "        reader = csv.DictReader(file, fieldnames=fieldnames)\n",
        "        header = next(reader)  # Read the header row\n",
        "        rows = list(reader)    # Read the remaining rows\n",
        "    \n",
        "    # Convert latitude and longitude to H3 hexagons\n",
        "    for row in rows:\n",
        "        # Extract latitude and longitude from the row\n",
        "        full_name = row['phoneme']\n",
        "        latitude = float(row['latitude'])\n",
        "        longitude = float(row['longitude'])\n",
        "        \n",
        "        # Convert latitude and longitude to H3 hexagon\n",
        "        h3_index = h3.geo_to_h3(latitude, longitude, resolution)\n",
        "        \n",
        "        # Replace latitude and longitude with H3 hexagon in the row\n",
        "        row[1] = h3_index\n",
        "        del row[2]  # Remove the longitude column\n",
        "    \n",
        "    # Create a new CSV file for output\n",
        "    output_path = 'output.csv'\n",
        "    with open(output_path, 'w', newline='') as file:\n",
        "        # Write the header row\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(header)\n",
        "        \n",
        "        # Write the modified rows\n",
        "        writer.writerows(rows)\n",
        "    \n",
        "    print(f\"Conversion complete. Output CSV file saved at: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "csv_path = Path(r\"C:\\Users\\YW195YV\\OneDrive - EY\\sandbox\\AutoML\\geo_sound_index.csv\")\n",
        "resolution = 12           # Specify the desired H3 resolution\n",
        "\n",
        "convert_coordinates_to_h3(csv_path, resolution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "QKSvgGqsYz5S",
        "outputId": "93e519ab-35b2-422d-b614-f43f389d962a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d1a3be7e7b0f>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mresolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m           \u001b[0;31m# Specify the desired H3 resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mconvert_coordinates_to_h3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-d1a3be7e7b0f>\u001b[0m in \u001b[0;36mconvert_coordinates_to_h3\u001b[0;34m(csv_path, resolution)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_coordinates_to_h3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Open the input CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Read the CSV data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfieldnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'phoneme,event,year,latitude,longitude'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\YW195YV\\\\OneDrive - EY\\\\sandbox\\\\AutoML\\\\geo_sound_index.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Path(r'https://data.sciencespo.fr/dataset.xhtml?persistentId=doi:10.21410/7E4/RDAG3O#')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btE_na0qY4ap",
        "outputId": "d0093109-113a-4910-d957-6ec767f73f54"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('https:/data.sciencespo.fr/dataset.xhtml?persistentId=doi:10.21410/7E4/RDAG3O#')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.sciencespo.fr/api/access/datafile/4432?gbrecs=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kAFTrTYhzQc",
        "outputId": "a6d459fe-13ac-497d-fac2-5dc07b207d67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-18 11:53:34--  https://data.sciencespo.fr/api/access/datafile/4432?gbrecs=true\n",
            "Resolving data.sciencespo.fr (data.sciencespo.fr)... 193.54.67.54\n",
            "Connecting to data.sciencespo.fr (data.sciencespo.fr)|193.54.67.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261545417 (249M) [application/x-gzip]\n",
            "Saving to: ‘4432?gbrecs=true’\n",
            "\n",
            "4432?gbrecs=true    100%[===================>] 249.43M  1.41MB/s    in 3m 8s   \n",
            "\n",
            "2023-05-18 11:56:43 (1.32 MB/s) - ‘4432?gbrecs=true’ saved [261545417/261545417]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import gzip\n",
        "import io\n",
        "from pathlib import Path\n",
        "import h3\n",
        "\n",
        "def parse_string_as_csv(string):\n",
        "  reader = csv.reader(io.StringIO(string))\n",
        "  return next(reader)\n",
        "      \n",
        "\n",
        "def clean_name(string):\n",
        "  return string.split(\"(\")[0].replace(\"_\",\" \")\n",
        "\n",
        "on_error = 0\n",
        "\n",
        "with gzip.open(Path('/content/names.csv.gz'), 'rb') as f:\n",
        "  with open('training_data.csv', 'w') as outfile:\n",
        "    writer = csv.DictWriter(outfile, fieldnames=['name','year','h3_index'])\n",
        "    writer.writeheader()\n",
        "    header = parse_string_as_csv(next(f).decode('utf-8'))\n",
        "    print(header)\n",
        "    for count, row in enumerate(f):\n",
        "      try:\n",
        "        value = row.decode('utf-8')\n",
        "        person = parse_string_as_csv(value)\n",
        "        p = dict(zip(header, person))\n",
        "        # name, birth_year, birth_latitude, birth_longitude\n",
        "        name = clean_name(p['name'])\n",
        "        birth_year = p['birth']\n",
        "        death_year = p['death']\n",
        "        try:\n",
        "          birth_coords = [float(p[x]) for x in ['bpla1', 'bplo1']]\n",
        "        except Exception as ee:\n",
        "            continue\n",
        "        try:\n",
        "          death_coords = [float(p[x]) for x in ['dpla1', 'dplo1']]\n",
        "        except Exception as ee:\n",
        "            continue\n",
        "        if death_year and all([x is not None and x != '' for x in birth_coords]):\n",
        "          #write death coords, death year\n",
        "          h3_index = h3_index = h3.geo_to_h3(*death_coords, resolution)\n",
        "          writer.writerow(dict(zip(['name','year','h3_index'], [name, year,h3_index])))\n",
        "        if birth_year and all([x is not None and x != '' for x in death_coords]):\n",
        "          #write birth coords, birth year\n",
        "          h3_index = h3_index = h3.geo_to_h3(*birth_coords, resolution)\n",
        "          writer.writerow(dict(zip(['name','year','h3_index'], [name, year,h3_index])))\n",
        "      except Exception as ee:\n",
        "        on_error += 1\n",
        "        if on_error % 1000 == 0:\n",
        "          print(on_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7GN96MQh3mW",
        "outputId": "f13d2f18-ebef-4327-e894-17d4b6eb653a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wikidata_code', 'birth', 'death', 'updated_death_date', 'approx_birth', 'approx_death', 'birth_min', 'birth_max', 'death_min', 'death_max', 'gender', 'level1_main_occ', 'name', 'un_subregion', 'birth_estimation', 'death_estimation', 'bigperiod_birth_graph_b', 'bigperiod_death_graph_b', 'curid', 'level2_main_occ', 'freq_main_occ', 'freq_second_occ', 'level2_second_occ', 'level3_main_occ', 'bigperiod_birth', 'bigperiod_death', 'wiki_readers_2015_2018', 'non_missing_score', 'total_count_words_b', 'number_wiki_editions', 'total_noccur_links_b', 'sum_visib_ln_5criteria', 'ranking_visib_5criteria', 'all_geography_groups', 'string_citizenship_raw_d', 'citizenship_1_b', 'citizenship_2_b', 'list_areas_of_rattach', 'area1_of_rattachment', 'area2_of_rattachment', 'list_wikipedia_editions', 'un_region', 'group_wikipedia_editions', 'bplo1', 'dplo1', 'bpla1', 'dpla1', 'pantheon_1', 'level3_all_occ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "from h3 import h3\n",
        "\n",
        "def find_top_h3_indexes(dataset_file, input_string):\n",
        "    # Step 1: Find the (up to) 500 closest strings\n",
        "    closest_strings = []\n",
        "\n",
        "    with open(dataset_file, 'r') as file:\n",
        "        csv_reader = csv.DictReader(file)\n",
        "        for row in csv_reader:\n",
        "            name = row['name']\n",
        "            if name.lower().startswith(input_string.lower()):\n",
        "                closest_strings.append(name)\n",
        "\n",
        "            if len(closest_strings) >= 500:\n",
        "                break\n",
        "\n",
        "    # Step 2: Find the top 3 most common h3_indexes for the closest strings\n",
        "    with open(dataset_file, 'r') as file:\n",
        "        csv_reader = csv.DictReader(file)\n",
        "        dataset = list(csv_reader)\n",
        "\n",
        "    h3_indexes = [row['h3_index'] for row in dataset if row['name'] in closest_strings]\n",
        "    top_h3_indexes = Counter(h3_indexes).most_common(3)\n",
        "\n",
        "    return top_h3_indexes\n"
      ],
      "metadata": {
        "id": "ELZW0pfzjppo"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "def plot_results_on_map(top_h3_indexes, name_string):\n",
        "    # Create a map centered at a specific location\n",
        "    map_center = [0, 0]  # Specify the latitude and longitude for the map center\n",
        "    map_zoom = 2  # Adjust the zoom level as needed\n",
        "    map_instance = folium.Map(location=map_center, zoom_start=map_zoom)\n",
        "\n",
        "    # Create a list to store the coordinates and weights\n",
        "    heat_data = []\n",
        "\n",
        "    # Add each h3 index as a heatmap point with the corresponding weight\n",
        "    for h3_index, weight in top_h3_indexes:\n",
        "        # Get the centroid coordinates of the h3 index\n",
        "        centroid = h3.h3_to_geo(h3_index)\n",
        "\n",
        "        # Append the coordinates and weight to the heat data list\n",
        "        heat_data.append(centroid + (weight,))\n",
        "\n",
        "    # Create a heatmap layer and add it to the map\n",
        "    heatmap_layer = HeatMap(heat_data)\n",
        "    heatmap_layer.add_to(map_instance)\n",
        "\n",
        "    # Add the name_string to the HTML file\n",
        "    name_html = f'<h3>{name_string}</h3>'\n",
        "    map_instance.get_root().html.add_child(folium.Element(name_html))\n",
        "\n",
        "    # Display the map\n",
        "    return map_instance\n",
        "\n",
        "def name_heatmap(name='Wolynko'):\n",
        "  pth = Path(r\"/content/training_data.csv\")\n",
        "\n",
        "  results = find_top_h3_indexes(pth, name)\n",
        "\n",
        "  map_plot = plot_results_on_map(results, name)\n",
        "  map_plot.save(f'/content/maps/{name}.html')"
      ],
      "metadata": {
        "id": "pCv5RbMZ6NLD"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['Cichocki','Kraszewski','Makarewicz','Stought']\n",
        "for name in names:\n",
        "  x = name_heatmap(name=name)\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "BI6GEj0u8YVN",
        "outputId": "d03b3e31-f9dc-4657-d747-7da101580113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "VkVsJpt_8jHu",
        "outputId": "80768fcd-f2bd-42d3-b093-5988f0d9dbdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('8c1f1d48864c7ff', 17), ('8c1f15ad31861ff', 15), ('8c1f8d7a498cdff', 8)]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/maps.zip /content/maps\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/maps.zip')\n"
      ],
      "metadata": {
        "id": "GaEqSe-L8scm",
        "outputId": "afaae802-870b-4146-c40c-202105c00666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/maps/ (stored 0%)\n",
            "  adding: content/maps/Makarewicz.html (deflated 66%)\n",
            "  adding: content/maps/Kraszewski.html (deflated 66%)\n",
            "  adding: content/maps/Stought.html (deflated 66%)\n",
            "  adding: content/maps/Cichocki.html (deflated 66%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e3522f40-996f-4bca-a473-0697b78e6f76\", \"maps.zip\", 5696)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "def plot_results_on_map(top_h3_indexes, name_string):\n",
        "    # Create a map centered at a specific location\n",
        "    map_center = [0, 0]  # Specify the latitude and longitude for the map center\n",
        "    map_zoom = 2  # Adjust the zoom level as needed\n",
        "    map_instance = folium.Map(location=map_center, zoom_start=map_zoom)\n",
        "\n",
        "    # Create a list to store the coordinates and weights\n",
        "    heat_data = []\n",
        "\n",
        "    # Add each h3 index as a heatmap point with the corresponding weight\n",
        "    for h3_index, weight in top_h3_indexes:\n",
        "        # Get the centroid coordinates of the h3 index\n",
        "        centroid = h3.h3_to_geo(h3_index)\n",
        "\n",
        "        # Append the coordinates and weight to the heat data list\n",
        "        heat_data.append(centroid + (weight,))\n",
        "\n",
        "    # Create a heatmap layer and add it to the map\n",
        "    heatmap_layer = HeatMap(heat_data)\n",
        "    heatmap_layer.add_to(map_instance)\n",
        "\n",
        "    # Add the name_string to the HTML file\n",
        "    name_html = f'<h3>{name_string}</h3>'\n",
        "    map_instance.get_root().html.add_child(folium.Element(name_html))\n",
        "\n",
        "    # Display the map\n",
        "    return map_instance\n",
        "\n",
        "# Example usage\n",
        "name_strings = ['Cichocki','Kraszewski','Makarewicz','Stought']\n",
        "\n",
        "for name_string, top_h3_indexes in zip(name_strings, ):\n",
        "    map_plot = plot_results_on_map(top_h3_indexes, name_string)\n",
        "    map_plot.save(f'{name_string}_heatmap.html')\n"
      ],
      "metadata": {
        "id": "gq27h7v5_Jzg"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_top_h3_indexes(dataset_file, )"
      ],
      "metadata": {
        "id": "IXwZyH2mASB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}